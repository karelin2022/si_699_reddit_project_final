{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609dd322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Make sure info.txt isn't uploaded to Github.\n",
    "# CSV files are too big to upload to Github. Send through Google Drive. Just upload .ipynb code to Github.\n",
    "\n",
    "import pandas as pd\n",
    "import schedule\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6be7c48-10eb-4721-a199-91c9c831b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('info.txt', 'r') as f:\n",
    "    info = f.readlines()\n",
    "        \n",
    "    # Public key\n",
    "    client_id = info[0].rstrip('\\n')\n",
    "\n",
    "    # Private key\n",
    "    secret_key = info[1].rstrip('\\n')\n",
    "    \n",
    "    auth = requests.auth.HTTPBasicAuth(client_id, secret_key)\n",
    "        \n",
    "    data = {\n",
    "    'grant_type': 'password',\n",
    "    'username': info[2].rstrip('\\n'),\n",
    "    'password': info[3]}\n",
    "    \n",
    "    headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "    \n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                   auth=auth, data=data, headers=headers)\n",
    "    \n",
    "    token = res.json()['access_token']\n",
    "    \n",
    "    headers = {**headers, **{'Authorization': f'bearer {token}'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19f2c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_reddit_data(subreddit, post_or_comment, csv_file):\n",
    "    link = 'https://oauth.reddit.com/r/' + subreddit + '/' + post_or_comment\n",
    "    res = requests.get(link, headers=headers,\n",
    "                  params={'limit': '100'})\n",
    "    \n",
    "    # Data in json form.\n",
    "    newest = res.json()\n",
    "    \n",
    "    newest_df = pd.DataFrame()\n",
    "    \n",
    "    # Grabs data on posts.\n",
    "    for post in newest['data']['children']:\n",
    "        if (post_or_comment == 'new') or (post_or_comment == 'hot'):\n",
    "            newest_df = newest_df.append({\n",
    "                'subreddit': post['data']['subreddit'],\n",
    "                'title': post['data']['title'],\n",
    "                'selftext': post['data']['selftext'],\n",
    "                'upvote_ratio': post['data']['upvote_ratio'],\n",
    "                'ups': post['data']['ups'],\n",
    "                'downs': post['data']['downs'],\n",
    "                'score': post['data']['score'],\n",
    "                'author': post['data']['author'],\n",
    "                'created_utc': datetime.datetime.fromtimestamp(post['data']['created_utc'])\n",
    "            }, ignore_index=True)\n",
    "        # Grabs data on comments.\n",
    "        elif (post_or_comment == 'comments'):\n",
    "            newest_df = newest_df.append({\n",
    "                'subreddit': post['data']['subreddit'],\n",
    "                'post_title': post['data']['link_title'],\n",
    "                'ups': post['data']['ups'],\n",
    "                'downs': post['data']['downs'],\n",
    "                'score': post['data']['score'],\n",
    "                'post_author': post['data']['link_author'],\n",
    "                'date': datetime.datetime.fromtimestamp(post['data']['created_utc']),\n",
    "                'num_comments': post['data']['num_comments'],\n",
    "                'comment_author': post['data']['author'],\n",
    "                'comment_text': post['data']['body']\n",
    "            }, ignore_index=True)\n",
    "\n",
    "    path = Path(csv_file)\n",
    "    \n",
    "    # If file is created already, append data and remove duplicates.\n",
    "    if path.is_file():\n",
    "        newest_df.to_csv(csv_file, mode='a', index=False, header=False)\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df = df.drop_duplicates()\n",
    "        df.to_csv(csv_file, index=False)\n",
    "    # If file is not created, create it with header.\n",
    "    else:\n",
    "        newest_df.to_csv(csv_file, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b702e2ad-423c-4605-8f79-3df8860820a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data():\n",
    "    # Antifeminism subreddit\n",
    "    fetch_reddit_data('Antifeminists', 'new', 'antifeminism_posts.csv')\n",
    "    fetch_reddit_data('Antifeminists', 'hot', 'antifeminism_hot.csv')\n",
    "    fetch_reddit_data('Antifeminists', 'comments', 'antifeminism_comments.csv')\n",
    "    \n",
    "    # Askfeminism subreddit\n",
    "    fetch_reddit_data('Askfeminists', 'new', 'askfeminism_posts.csv')\n",
    "    fetch_reddit_data('Askfeminists', 'hot', 'askfeminism_hot.csv')\n",
    "    fetch_reddit_data('Askfeminists', 'comments', 'askfeminism_comments.csv')\n",
    "    \n",
    "    # Feminism subreddit\n",
    "    fetch_reddit_data('Feminism', 'new', 'feminism_posts.csv')\n",
    "    fetch_reddit_data('Feminism', 'hot', 'feminism_hot.csv')\n",
    "    fetch_reddit_data('Feminism', 'comments', 'feminism_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cbd4f4-6601-41b1-a0e1-bd7227b596d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_posts_comments(post_file, comments_file, csv_file):\n",
    "    posts_df = pd.read_csv(post_file)\n",
    "    comments_df = pd.read_csv(comments_file)\n",
    "    \n",
    "    merged_df = pd.merge(\n",
    "        posts_df,\n",
    "        comments_df,\n",
    "        how=\"outer\",\n",
    "        left_on=['subreddit', 'title', 'author'],\n",
    "        right_on=['subreddit', 'post_title', 'post_author'],\n",
    "        indicator=True\n",
    "    ).rename(columns={'selftext': 'post_selftext', \n",
    "                      'upvote_ratio': 'post_upvote_ratio', \n",
    "                      'ups_x': 'post_ups', \n",
    "                      'downs_x': 'post_downs', \n",
    "                      'score_x': 'post_score', \n",
    "                      'created_utc': 'post_date', \n",
    "                      'ups_y': 'comment_ups', \n",
    "                      'downs_y': 'comment_downs', \n",
    "                      'score_y': 'comment_score', \n",
    "                      'date': 'comment_date', \n",
    "                      'num_comments': 'post_num_comments'}).drop(['title', 'author'], \n",
    "                                                                 axis=1)\n",
    "\n",
    "    reorganized_df = merged_df[['subreddit', \n",
    "                                'post_title',\n",
    "                                'post_selftext',\n",
    "                                'post_author',\n",
    "                                'post_date',\n",
    "                                'post_num_comments',\n",
    "                                'post_upvote_ratio',\n",
    "                                'post_ups',\n",
    "                                'post_downs',\n",
    "                                'post_score',\n",
    "                                'comment_text',\n",
    "                                'comment_author',\n",
    "                                'comment_date',\n",
    "                                'comment_ups',\n",
    "                                'comment_downs',\n",
    "                                'comment_score',\n",
    "                                '_merge']]\n",
    "\n",
    "    reorganized_df.to_csv(csv_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb008dc-103d-48c8-9222-20e614811ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging():\n",
    "    merge_posts_comments('antifeminism_posts.csv', 'antifeminism_comments.csv', 'antifeminism_posts_comments.csv')\n",
    "    merge_posts_comments('antifeminism_hot.csv', 'antifeminism_comments.csv', 'antifeminism_hot_comments.csv')\n",
    "    \n",
    "    merge_posts_comments('askfeminism_posts.csv', 'askfeminism_comments.csv', 'askfeminism_posts_comments.csv')\n",
    "    merge_posts_comments('askfeminism_hot.csv', 'askfeminism_comments.csv', 'askfeminism_hot_comments.csv')\n",
    "    \n",
    "    merge_posts_comments('feminism_posts.csv', 'feminism_comments.csv', 'feminism_posts_comments.csv')\n",
    "    merge_posts_comments('feminism_hot.csv', 'feminism_comments.csv', 'feminism_hot_comments.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d872f6da-8f1e-410d-9afc-7519a4367efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule.every(1).hours.do(get_all_data)\n",
    "schedule.every(1).days.do(merging)\n",
    "\n",
    "# Comment to stop schedule\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    # schedule.cancel_job(get_all_data)\n",
    "    time.sleep(5)\n",
    "\n",
    "# Uncomment to stop schedule\n",
    "# schedule.cancel_job(get_all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a0328b-ba89-4ebc-95bb-eee822a63340",
   "metadata": {},
   "source": [
    "## Create a function that takes in a csv_file and does NLP analysis on post_title, post_selftext, and comment_text. Run function for all 6 csv files ending in either hot_comments or posts_comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a1039-fc82-4973-9236-6310ac91d372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
